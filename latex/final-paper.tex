\documentclass[11pt]{article}
\usepackage{common}
\usepackage{dirtree}
\pagestyle{plain}
\title{CS182 Final Project}
\author{Nikhil Suri and Soumil Singh}
\begin{document}
\maketitle{}


\section{Introduction}
The initial goal in undertaking this project was to investigate the possible application of Q-learning for the game of Tetris. For an explanation of the rules of Tetris, read appendix (insert appendix). More specifically, we wanted to apply Q-learning techniques to create an agent which could learn how to optimally play the game i.e. optimally 'clear lines' and survive for as long as possible. Our goal was to compare the effectiveness of exact Q-learning techniques (model-free learning) against approximate Q-learning techniques (model-based learning) for different specifications of the game, that is, for different sizes of the game board and under different models of rewards. We hypothesised that in general an approximate Q-learning learning agent would perform better than an exact Q-learning agent predicated on the reasoning that our defined state-space would be very large, and thus our exact Q-learning agent wouldn't sufficiently explore the state-space to conduct its learning. To elucidate this comparison, we also had the goal of comparing the effectiveness of the two different agents for different boards (i.e. differently sized state-spaces), different learning parameters (i.e. different learning rates $\alpha$ and different random-action selection rates for epsilon greedy approach $\epsilon$), and different heuristics/features for the approximate Q-learning agent. After conducting analysis on these manifold aspects, we have the goal of drawing conclusions regarding what an optimal Q-learning agent for this game might look like.

The Q-learning algorithms we applied, detailed in the Approach section, were fundamentally the same algorithms we learnt in class. However, we created our own representation for states and actions for the tetris game. Furthermore, we defined our own features for approximate Q-learning, and implemented our own method for testing both agents. Our baseline agent we tested against was the random-action agent, which picks any legal action with equal probability. We utilised an existing pygame version of tetris, with a pre-defined implementation of the tetris board, pieces, and user-interaction (key-board presses) functionality; we built upon this framework to implement our Q-learning agents by defining our own representation for states, legal actions, rewards, storage of Q-values and so forth.

\section{Scope}

\subsection{Background and Related Work}
To apply these Q-learning approaches to tetris, we used an existing pygame representation of the tetris game; thus, we had access to code which defined the board, pieces, keyboard-moves, and all visual representation. We then created our own representations for states and actions (the definitions of which can be found in the section titled 'problem specification') for which our agents would learn with. We then applied the feature based and exact Q-learning algorithms we learnt during this course. We utilised a similar structure to that in the Reinforcement Learning problem set to define the 'RLAgent' class.

look at papers we used for the project proposal.

For instance, \cite{hochreiter1997long}.


\subsection{Problem Specification}
The specific problem that we are investigating is how well reinforcement learning, through exact and approximate q learning, can be applied to the game of Tetris. To apply reinforcement learning to Tetris, we must have a formal definition of the game, the state space, and the action set of Tetris.

\subsubsection{Game}
The game of Tetris is played on a board of width $w$ and height $h$. This board initially starts off empty. During each turn, the player is given a piece to place on the board that begins falling from the top of the board. The player can rotate and move this falling piece from side to side. As the player places more pieces on the board, the stack of pieces rises higher and higher. If the stack of pieces rises too high and the player cannot place a piece anywhere on the board, then the game is over. The player attempts to keep the stack of pieces on the board low by clearing lines: every time the player fills an entire row of blocks, that line is cleared from the game board and all blocks above that row shift down. The player also scores points by clearing lines. The goal of the game is score as many points as possible. This is done by clearing lines, and thus by continuing to play for as long as possible.

\bigskip

In addition to knowing the entire board configuration and falling piece, the player is told what the next piece will be (pieces are assumed to spawn randomly). The player also has the option to place a piece in "reserve" and save it for later. The idea behind this is that by putting a piece in reserve, the player can plan to use the piece to clear lines later or temporarily escape from a situation where there are no locations that the player deems to be "good" to place the piece.

\bigskip

For our project, we have slightly simplified Tetris. In real Tetris, part of the challenge of placing pieces on the board (in addition to choosing the best rotation and column to drop the piece in) is actually getting the piece to the column in which you want to drop it. Pieces spawn in the center at the top of the board and fall at a certain rate. The rate at which pieces fall is determined by how many lines the player has cleared. Once a falling piece touches the stack of pieces already on the board, it becomes stuck, and the player can no longer move it. In some versions of Tetris, there is additional complexity where, for a short time after the piece touches the stack, the player can continue to move it from side to side and rotate it. Currently, we do not support these features in our implementation of the game. We also currently do not allow the agent to hold a piece in reserve. In our implementation, the agent is simply given a piece which it can place anywhere atop the current stack of pieces on the board.

\subsubsection{State Space}
Tetris has a huge state space. Each square on the board can either have a block in it or not. If we let $w$ be the width of the board and $h$ be the height of the board, then the number of possible board configurations is $$2^{w\cdot h}$$
Now, an important observation is that we will never reach a board configuration where there are rows which are full of blocks (these rows will be cleared, according to the rules of Tetris). Therefore, this is a counting problem: the number of board configurations which have a single row of full blocks is
$$\binom{h}{1}2^{w\cdot(h-1)}$$
The number of configurations which have 2 rows of full blocks is
$$\binom{h}{2}2^{w\cdot(h-2)}$$
and so on. Therefore, in reality the number of possible states is
$$2^{wh} - \sum_{k=1}^{h}\binom{h}{k}2^{w\cdot(h-k)}$$
However, we note that this reduction is really of no help. A typical board in Tetris is 10x20 squares, which by our calculation has
$$2^{200} - \sum_{k=1}^{20}\binom{20}{k}2^{10\cdot(20-k)}=1575259648227583387903265739476252776198605528180667457712127$$
possible states. Even on a smaller board with dimensions 5x8, we will still have
$$2^{40} - \sum_{k=1}^{8}\binom{8}{k}2^{5\cdot(8-k)}=792614637311$$
possible states. Clearly, training an exact Q Learning agent which stores entire board configurations as states is not feasible for Tetris. We must also consider that a state does not consist solely of a board configuration; in Tetris we are also know what the falling piece and next piece are. Since there are 7 pieces in Tetris, our state space is already larger by a factor of 49. Therefore, an exact Q Learning agent requires a much more concise way of representing states. Here, we introduce the concept of the "top line" of a board. The top line of a board is the $w$ (where $w$ is the width of the board) free spaces (not occupied by blocks) which are at the top of each column of the board. This greatly reduces our state space. To count the number of possible top line configurations that there are in a board of width $w$ and height $h$, let us first consider the number of ways to place blocks in a single column. In a single column, there are $h$ ways to arrange blocks ($h$ locations to place a block). In 2 columns, there are $h^2$ ways to arrange blocks. We need to correct for counting complete rows. The number of permutations where all of the blocks are in the same row is $h$. Therefore, the total number of top line configurations that we will have is
$$h^w - h$$
For a typical board of size 10x20 the number of top line configurations will be
$$20^{10} - 20 = 10239999999980$$
This is still quite large. However, for a smaller board of size 5x8 the number of top line configurations will be
$$8^5 - 8 = 32760$$
Now this is a feasible state space size! We can make this even smaller if we consider the effect of "normalization". By normalization, we mean that we can "normalize" a (row, col) top line of $((2,0),(2,1),(1,2),(1,3),(2,4))$ to $((1,0),(1,1),(0,2),(0,3),(1,4))$. The key observation that leads to normalization is that from the \textit{perspective of placing pieces}, the two top lines are the same. We would want the same action that occurs in the latter top line to occur in former top line, because the agent should be selecting the optimal action in both cases. Normalization therefore has the effect of always reducing a top line to one which has at least one column which is empty. To calculate the effect which normalization has, let us consider the case where there is one empty column on the board. The number top lines we can have is
$$h^{w-1}\binom{w}{1}$$
If there are two empty columns, the number of top lines we can have is
$$h^{w-2}\binom{w}{2}$$
Therefore, with normalization, the total number of top lines we can have for a board with width $w$ and height $h$ is
$$\sum_{k=1}^{w-1}h^{w-k}\binom{w}{k}$$
On a typical board of size 10x20 the number of normalized top lines will be
$$\sum_{k=1}^{9} 20^{10-k}\binom{10}{k} = 6439880978200$$
On the smaller board of size 5x8 the number of normalized top lines will be
$$\sum_{k=1}^{4} 8^{5-k}\binom{5}{k} = 26280$$
Adding in the falling piece and next piece, we see that on a board of size 5x8 the size of the state space will be
$$(49)(26280) = 1287720$$
Although large, this state space is still manageable for exact learning. For approximate learning, we do not need to worry about reducing the state space so much. In approximate learning, we do not need to store a table of (state, action) pairs. We determine the value of a state by using heuristics which are calculated based on the current configuration of the entire board, the falling piece, and the next piece. Therefore, our states for approximate q learning are simply the entire board configuration, the falling piece, and the next piece.

\subsubsection{Action Set}
Given a state, we have defined the set of legal actions for that state as (rotation, column) pairs. Here, rotation is a number that represents which rotation a falling piece is in. Column is a number that represents which column we are dropping the piece now, with respect to the top left corner of that piece in its rotation. Once an action is chosen, we will set the rotation of the falling piece, move it over to the column which we want to drop it down, and drop it as far down as it can go in that column. The maximum number of rotations for any piece is 4. In a board of width $b$, the maximum number of columns which we can drop a piece down is  $b$. Therefore, there are at most $4b$ actions for every piece. Since there are 7 different pieces in Tetris, the number of different actions there are is at most $28b$. We use the same action set for approximate learning as we do for exact learning.

\section{Approach}

\subsection{Game Implementation}
The implementation of the game itself is adapted from "Tetronimo" (a Tetris clone) by Al Sweigart. Many of the functions used to draw objects on the screen and manipulate the game board (such as clearing lines and dropping pieces) were written by him. We adapted the game to be Object-Oriented: each piece object is generated by an object called the "piece generator". The board object then has functions to manipulate the board itself. The game board itself is stored as a column-wise list of lists. For our Exact Learning agent, we store states as tuples of the board's top line, the shape of the falling piece, and the shape of the next piece. For our Approximate Learning agent, we store states as tuples of the current board, the shape of the falling piece, and the shape of the next piece. For both agents, actions are stored as tuples of (rotation, column) pairs.

\subsection{Algorithms}
The main algorithms that we used were the Exact Q Learning and Approximate Q Learning algorithms. As defined in the lecture notes, the Exact Q Learning algorithm stores a lookup table of (state, action) pairs with their associated Q-values. To update Q-values, we use the following algorithm where $s$ and $a$ are the previous state and action, $s'$ is the new state, $r$ is the observed reward, $\gamma$ is the discount factor, and $\alpha$ is the learning rate:
\begin{algorithm}
  \begin{algorithmic}
    \Procedure{updateQValues}{$s, a, s', r$}
        \State{$sample \gets r + \gamma\max_{a'}Q(s', a')$}
        \State{$Q(s,a) \gets Q(s,a)(1 - \alpha) + \alpha\cdot sample $}
    \EndProcedure{}
  \end{algorithmic}
  \caption{Exact Q Learning Q-value Update}
\end{algorithm}\\
As defined in the lecture notes, the approximate Q Learning algorithm stores a table of weights associated with pre-defined features. These features are obtained by evaluating certain heuristics on a (state, action) pair. The Q-value of a state thus becomes the dot product of the feature and weight vectors:
$$Q(s,a) = w_1f_1(s,a) + w_2f_2(s,a) + \ldots + w_nf_n(s,a)$$
To update values for the weights, we use the following algorithm where where $s$ and $a$ are the previous state and action, $s'$ is the new state, $r$ is the observed reward, $\gamma$ is the discount factor, and $\alpha$ is the learning rate:
\begin{algorithm}
  \begin{algorithmic}
    \Procedure{updateWeights}{$s, a, s', r$}
        \State{$difference \gets \alpha(r + \gamma\max_{a'}Q(s', a') - Q(s,a))$}
        \For{i = 1:n}
            \State{$w_i \gets w_i + f_i(s,a)\cdot difference $}
        \EndFor{}
    \EndProcedure{}
  \end{algorithmic}
  \caption{Approximate Q Learning Weights Update}
\end{algorithm}\\
For our implementation, we adopted much of the code we used for problem set 3 on reinforcement learning in pacman. To store our Q-value and weight lookup tables, we used the \texttt{Counter} object defined in \texttt{util.py}.

\bigskip

We also utilized $\epsilon$-greedy action selection and a depth-first search for one of the heuristics that we defined for our approximate learning agent.

\subsection{Rewards}
We used 3 different reward models to test our agents. Below is a description of each. We will discuss the results and effectiveness of each in the results section:
\begin{enumerate}
    \item The first rewards model we defined used a "living penalty" of -1 for any move that did not result in a line clear. Line clears were scaled up by 1000 so that clearing a single line would result in a reward of 1000, clearing 2 lines would result in a reward of 2000, etc. There was no "losing penalty" enacted when the agent lost the game. The idea behind this model was that it might incentivize the agent to make line clears as fast as possible.
    \item The second rewards model we defined has no penalty for moves that do not result in line clears. There is only a penalty for losing of -0.5. Line clears are scaled down by 1000. Therefore, clearing a line results in a reward of 0.001, clearing 2 lines results in a reward of 0.002, etc. The idea behind this model was that it could incentivize the agent to stay alive for as long as possible.
    \item The third rewards model we defined combines the living penalty for making moves that do not result in line clears with the penalty for losing and the scaled down penalty for clearing lines. We also kept the losing penalty of -0.5 from model 2. Given a board width of $b$, we set the living penalty to be $-0.001/b$. The idea behind this is that the agent should be clearing lines about as often as they get enough pieces to fill up the width of the board.
\end{enumerate}

\subsection{Heuristics}
We defined and tested several heuristics for approximate learning. In our feature extractor, we calculated features given a state-action pair. To do this, we made a copy of the current board-representation (so that by performing the given 'action', the actual board was not edited) and performed the action on this copy. Then, we calculated heuristics on this board-copy to assess what our 'features' would be if we performed a particular action on a particular current board representation. Thus, given a (state, action) pair, our heuristics  were calculated as follows:
\begin{enumerate}
    \item \textbf{Highest Point:}
    This refers to the highest point on the current 'stack' on the board, where the stack refers to the amalgamation of pieces below the empty space at the top of the board. We anticipated that our approximate agent would learn negative weights for this feature, because a larger 'highest point' would, by proxy, imply a higher 'piece stack', which would mean our agent is closer to breaching the top of the board and dying. (we should get some images to display these)
    \item \textbf{Aggregate Height:}
    This is the sum of all the highest points of each column of the current stack of blocks that are on the board. This is another feature which we would expect our agent to learn a negative weight for, because a higher aggregate height means that our agent is closer to the top of the board and losing.
    \item \textbf{Holes:}
    The number of 'Holes' refers to the number of blank-spaces below the top line of our 'piece stack'. They define the empty spaces that are currently inaccessible to our agent, i.e. there exists a filled space at some height above the height of the 'hole'. We anticipated that our approximate Q-learning agent would learn negative weights for this feature, because a greater number of holes implies our 'piece-stack' is not optimally compressed; this means that it is 1) more likey that our 'piece-stack' is taller, and 2) future line-clears could be more difficult for our agent. In both cases, it would seem our agent should seek to minimize this feature.
    \item \textbf{Squared Hole Size:}
    The idea behind squared hole size was to not just naively count the number of 1x1 empty spaces on the board. Squared hole size redefines a "hole" on the board as any group of empty spaces that are adjacent to each other on the board and below the top line of the board (and thus inaccessible). To find the size of each "hole" (using this new definition of hole), we keep a set of all of the 1x1 holes on our board that are below the top line. Then, we run a DFS on this set of 1x1 holes, counting the size of each hole that consists of connected 1x1 holes. We catch the case where our DFS ventures above the top line in some other column. We also expected our agent to learn a negative weight for this feature because a higher squared hole size means that it is harder to clear lines on the board.
    \item \textbf{Lines Cleared:}
    This feature refers to the number of complete Lines that would be cleared after taking the specified action $\alpha$, where a 'complete line' refers to a row on the tetris board in which every space is filled by a piece.
    \item \textbf{Height Difference:}
    This feature measures the "bumpiness" of the board. This is defined as the average difference between the heights of all adjacent columns on the board. We expected our agent to learn a negative weight for this feature because a very bumpy board would make it hard to neatly fill rows and clear lines on the board.
\end{enumerate}

\section{Experiments}

\subsection{Methodology}
Testing our agents took up a significant portion of our time to complete this project. The challenge of testing our exact learning agents was that there are so many Q-values, so learning an effective policy could take hundreds of thousands of iterations. The challenge of testing our approximate learning agents was that there were many features to compare against each other, and sometimes our agents learned weights for features so well that they continued to play for a very long time during testing! Therefore, to obtain more visibility through our training and testing runs, we trained and tested our agents in sets of games and recorded average scores over training and testing episodes. For example, we often trained our exact learning agents on sets of 50000 games, checked and recorded our results, and then loaded in the learned q-values and tested for another 50000 games. We did the same for our approximate agents, though they needed many fewer training games. To export and import q-values and weights across training and testing runs, we used the python library \texttt{cPickle}.

\bigskip

We wanted to compare the performance of our rewards models and agents (random, exact, and approximate) against each other. However, before we could accurately compare results across models, we had to gain a better understanding of what kinds of learning parameters and board sizes would work best for our agents. Therefore, much of our testing also involved varying board sizes and learning parameters.

\subsection{Results}

\subsubsection{Exact vs. Approximate Learning}
If there is one major takeaway from this project, it is that approximate learning completely outperforms exact learning for Tetris. Typically our exact learning agents took around 500000 training games to get to a point where they could clear just 7 lines per game on average on a 5x8 board. Our approximate agents were able to clear, on average, 20 lines per game on a 5x8 board after only 500 training games.

\subsubsection{Learning Parameters and Board Sizes}


\subsubsection{Rewards Model Comparisons}
What were results for exact q learning on models? (use the graph we created for the poster)
what were results for approx q learning on models? (should all be bad results)
These should all have graph associated with them:
- iterations vs rewards for different learning parameters
- approx vs exact on iterations vs rewards
- iterations vs rewards for different board sizes (for exact and approx)

For algorithm-comparison projects: a section reporting empirical comparison results preferably presented graphically.


\section{Discussion}

Summary of approach and results. Major takeaways? Things you could improve in future work? Ways to extend the project?

\appendix

\section{System Description}
The main file structure of our project is as follows:

\bigskip

\dirtree{%
    .1 tetris-agent/.
    .2 docs/.
    .2 img/.
    .2 latex/.
    .2 src/.
    .2 values/.
}

\bigskip

The \texttt{docs} folder contains PDF documents for our project proposal, update, and final report. The \texttt{img} folder contains any images (including graph images) that we have included in any of our PDF documents. The \texttt{latex} folder contains latex files used to generate the PDF documents. All of our code resides in the \texttt{src} folder, and the \texttt{.pickle} files that we used to store learned Q-values and weights for training sessions are held in the \texttt{values} folder.

\bigskip

To run our code, first a user must \texttt{cd} into the \texttt{src} directory. The driver program for training, testing, and playing agents is \texttt{tetroid.py}. Much of our system is adapted from problem set 3 (on reinforcement learning), so there is a host of command line options that can be used. We support the following options (a detailed usage string can also be generated by running \texttt{python tetroid.py --help}):
\begin{enumerate}
    \item \texttt{-h, --help}: This option generates the help message and exits the program.
    \item \texttt{-a AGENT, --agent=AGENT}: Selects the agent type to use. There are 3 agents to choose from: \texttt{RandomAgent}, \texttt{ExactQAgent}, and \texttt{ApproximateQAgent}. If this option is not provided, then the default agent that will be selected is \texttt{RandomAgent}.
    \item \texttt{-g AGENTARGS, --agentArgs=AGENTARGS}: This specifies initial values to be sent to the agent. Initial values should be specified only for agent options which are different than their default values. These should be comma separated. e.g. \texttt{opt1=val1,opt2,opt3=val3}. The options that can be set for an agent are (written with their default values) \texttt{numTraining=10, numTesting=10, gamesPerEpisode=10, epsilon=0.5, alpha=0.5, epsilonDelta=0, alphaDelta=0, gamma=1}. \texttt{numTraining} and \texttt{numTesting} set the number of training and testing episodes to run. \texttt{gamesPerEpisode} sets the number of games to play per training/testing episode. \texttt{epsilon}, \texttt{alpha}, and \texttt{gamma} set the learning parameters for the agent. \texttt{epsilonDelta} and \texttt{alphaDelta} set the per-episode decrease rate for \texttt{alpha} and \texttt{epsilon}.
    \item \texttt{-x FILENAME, --export=FILENAME}: Setting this option will export the learned q-values/weights to \texttt{tetris-agent/values/FILENAME.pickle} after training.
    \item \texttt{-l FILENAME, --load=FILENAME}: Setting this option will load in previously learned q-values/weights from \texttt{tetris-agent/values/FILENAME.pickle}. We can use these loaded values as a starting point for further training, testing, or empirically evaluating the agent by watching it play.
    \item \texttt{--train}: Train the agent. This is set to \texttt{false} by default.
    \item \texttt{--test}: Test the agent. This is set to \texttt{false} by default.
    \item \texttt{-p PROGRESSTRACKER, --progress=PROGRESSTRACKER}: This will set the rate at which we show the completion of games during training or testing sessions. This is set to 1 by default.
    \item \texttt{-n, --no-play}: By default, the agent will begin playing the game after training and testing is over. This flag will tell the system to NOT show the agent playing the game.
    \item \texttt{-b BOARD\_DIM, --board=BOARD\_DIM}: Set the board dimensions. These should be given as \texttt{WIDTHxHEIGHT}. The board dimensions are set by default to be 10x20.
\end{enumerate}
Some example commands are below:
\begin{enumerate}
    \item \texttt{python tetroid.py -a ApproximateQAgent -l 300TrainFeatures}\\ Running this command will load in the file \texttt{300TrainFeatures.pickle} from the \texttt{values} directory and show our Approximate Q Learning Agent playing the game using these weights on a 10x20 board.
    \item \begin{verbatim} python tetroid.py
    -a ExactQAgent
    -g numTraining=1,numTesting=5,gamesPerEpisode=10000,alpha=0.05
    -p 1000 --train --test -n -b 5x8 -x foo
    \end{verbatim}
    One of our most frequently used commands when training our Exact Learning agents. This will begin training an Exact Q Learning agent on a 5x8 board from scratch. It will train over 50000 games and test over 10000 games using learning parameters of $\alpha=0.05$, $\epsilon=0.5$, and $\gamma=1$ (note how we only set option for the agent which differ from their default values), showing progress after every 1000 games. After training, it will export the values it learned to a file called \texttt{foo.pickle}, which will be stored in the \texttt{values} directory. After testing, it will display average rewards over every set of 10000 games (through training and testing). Finally, the program will exit without showing the agent playing.
\end{enumerate}

\section{Group Makeup}
This project was completed by Nikhil Suri and Soumil Singh. Both were responsible for and contributed to implementation, experimentation, analysis, and documentation of the entire system.

\bibliographystyle{plain}
\bibliography{final-project}

\end{document}
